{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boilerplate data preparation and computation\n",
    "\n",
    "When detecting text-reuse with passim, one can start by identifying and eliminating the boilerplate to allow to remove superfluous data from the processing.\n",
    "\n",
    "This notebook contains the code to perform various tasks relating to this:\n",
    "- Preparing the input data directory, only containing data that should be part of boilerplate detection\n",
    "- Light postprocessing of the boilerplate output and preparation of the actual text-reuse detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import jq\n",
    "import json\n",
    "from typing import Any\n",
    "from smart_open import open as smart_open_function\n",
    "import jsonlines\n",
    "import pickle\n",
    "\n",
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "from impresso_commons.utils.s3 import get_s3_resource\n",
    "from impresso_commons.utils.s3 import IMPRESSO_STORAGEOPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_reuse_dir = '/scratch/piconti/impresso/text_reuse'\n",
    "all_rebuilt_data_path = \"rebuilt_data\"\n",
    "bp_rebuilt_data_path = \"rebuilt_data_for_bp\"\n",
    "\n",
    "input_dir = os.path.join(text_reuse_dir, all_rebuilt_data_path)\n",
    "output_dir = os.path.join(text_reuse_dir, bp_rebuilt_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the copied Rebuilt data for Boilerplate detection with passim\n",
    "\n",
    "Before text-reuse detection with passim can be run, one must first run the tool in boilerplate mode to identify segments of text that should be excluded from the text-reuse search.\n",
    "\n",
    "However not all the corpus should be confronted to boilerplate detection, and titles without any article-level segmentation should not be considered.\n",
    "\n",
    "This small notebook aims to copy the wanted data (one that has article-level segmentation) to a new directory, where the files will be all together, as opposed to separated by title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_no_bp = [\n",
    "    \"FedGazDe\", \"FedGazFr\", \"NZZ\", \"handelsztg\", \"arbeitgeber\", \"ACI\", \"AV\", \"Bombe\", \"Cancoire\", \"Castigat\", \"Charivari\", \"CharivariCH\", \"CL\", \n",
    "    \"Croquis\", \"EM\", \"esta\", \"FAM\", \"FAMDE\", \"FAN\", \"FAV1\", \"Fronde\", \"GAVi\", \"Grelot\", \"Griffe\", \"Guepe1851\", \"Guepe1887\", \n",
    "    \"JH\", \"JV\", \"JVE\", \"JY2\", \"MB\", \"ME\", \"MESSAGER\", \"Moniteur\", \"NS\", \"NV\", \"NV1\", \"NV2\", \"OBS\", \"ouistiti\", \"pages\", \"PAT\", \"PDL\", \"PJ\", \"PS\", \"RLA\", \"TouSuIl\", \"VVS\", \"VVS1\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copied, not_copied = [], []\n",
    "for path, dir, files in os.walk(input_dir):\n",
    "    if len(dir)==0:\n",
    "        if path.split('/')[-1] in titles_no_bp:\n",
    "            not_copied.extend(files)\n",
    "            print(f\"Not copying {path.split('/')[-1]} files since it has no article segmentation.\")\n",
    "        else:\n",
    "            print(f\"Copying {path.split('/')[-1]} files...\")\n",
    "            for file in tqdm(files):\n",
    "                src_path = os.path.join(path, file)\n",
    "                dest_path = os.path.join(output_dir, file)\n",
    "                shutil.copy(src_path, dest_path)\n",
    "                copied.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(copied), len(not_copied), len(copied) + len(not_copied)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload the current boilerplate output to s3\n",
    "\n",
    "For traceability, upload all the contents of the /out.json directory to S3 under a boilerplate partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_staging_bucket = \"41-processed-data-staging\"\n",
    "partition = \"text-reuse/text-reuse_v1-0-0/boilerplate/out.json\"\n",
    "out_jsons_dir = os.path.join(text_reuse_dir, \"passim_bp_output/out.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = get_s3_resource()\n",
    "bucket = s3.Bucket(s3_staging_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm(os.listdir(out_jsons_dir)):\n",
    "    if filename.endswith('json'):\n",
    "        filepath = os.path.join(out_jsons_dir, filename)\n",
    "        key_name = os.path.join(partition, filename)\n",
    "        bucket.upload_file(filepath, key_name)\n",
    "        #print(\"Uploaded %s to s3: %s\", filepath, key_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the bp.pkl file from the boilerplate output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First check the contents of some resulting jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_jsons_dir = os.path.join(text_reuse_dir, \"passim_bp_output/out.json\")\n",
    "\n",
    "os.listdir(out_jsons_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(f_path: str) -> dict:\n",
    "    lines = []\n",
    "    with open(f_path, mode=\"r\", encoding='utf-8') as f_in:\n",
    "        for line in f_in:\n",
    "            lines.append(json.loads(line))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually create the pb.pkl dataframe\n",
    "\n",
    "From looking at examples the following heuristics have been devised:\n",
    "- Only entries with the field `\"src\"` are actually boilerplate.\n",
    "  - In the format: `{\"id\": \"BDC-1839-03-18-a-i0005_1658_1952\", \"src\": {\"id\": \"BDC-1839-03-16-a-i0004\", \"start\": [...]`\n",
    "- For each entry with the field `\"src\"`:\n",
    "  - Two text passages are linked: the value of `id` and the value of `src.id`\n",
    "  - The value of `id` will often have additional values appended after the usual CI id (\"_1658_1952\" here). These should be removed\n",
    "  - Both ids (fields `id` and `src.id`) should be included in the bp.pkl output as separate rows\n",
    "- All rows should also have a column `\"is_boilerplate\"` set to `True`.\n",
    "\n",
    "The actual processing for this step of creating the bp.pkl dataframe has been moved to the `postprocess_boilerpalte.py` script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the resulting file to S3\n",
    "\n",
    "The upload of this dataframe has also been outsourced to the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_pkl_out_filepath = os.path.join(text_reuse_dir, \"bp.pkl\")\n",
    "s3_staging_bucket = \"41-processed-data-staging\"\n",
    "pb_partition = \"text-reuse/text-reuse_v1-0-0/boilerplate/\"\n",
    "\n",
    "s3 = get_s3_resource()\n",
    "bucket = s3.Bucket(s3_staging_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_pkl_out_filepath = os.path.join(text_reuse_dir, \"bp.pkl\")\n",
    "bp_key_name = os.path.join(pb_partition, \"bp.pkl\")\n",
    "bucket.upload_file(pb_pkl_out_filepath, bp_key_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug: Filtering the duplicates and uploading updated pkl to s3\n",
    "\n",
    "The first iteration of the bp.pkl dataframe was created without filtering out the duplicated ids. \n",
    "This caused some issues during the filtering, among other things due to the large size of the dataframe.\n",
    "The code filtering out duplicated ids had now also been added to the script\n",
    "\n",
    "Note: When using `compute()` after filtering, the output is of type pd.DataFrame and not dd.\n",
    "Trying both options and seeing which is best. --> decision was to keep it as a dask dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_staging_bucket = \"\" # todo fill in\n",
    "pb_partition = \"text-reuse/text-reuse_v1-0-0/boilerplate/\"\n",
    "bp_dd_filename = '' # todo fill in with desired partition subpath\n",
    "dd_s3_path = os.path.join(\"s3://\", s3_staging_bucket, pb_partition, bp_dd_filename)\n",
    "dd_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the full df\n",
    "bp_df = pd.read_pickle(dd_s3_path, storage_options=IMPRESSO_STORAGEOPT).repartition(npartitions=2082).drop(columns=[\"is_boilerplate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_df_filter = bp_df['id'].value_counts().map(lambda x: x > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_full_count = bp_df.count().compute()\n",
    "not_dupl = bp_df_filter[~bp_df_filter].count().compute()\n",
    "dupl = bp_df_filter[bp_df_filter].count().compute()\n",
    "\n",
    "bp_full_count, not_dupl, dupl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dup_bp = bp_df.drop_duplicates(subset=['id']).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dup_bp.head(), filtered_dup_bp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_reuse_dir = '/scratch/piconti/impresso/text_reuse'\n",
    "bp_out_filename = None # TODO fill in\n",
    "filtered_bp_pkl_out_filepath = os.path.join(text_reuse_dir, bp_out_filename)\n",
    "filtered_bp_pkl_out_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filtered_bp_pkl_out_filepath, 'wb') as handle:\n",
    "    pickle.dump(filtered_dup_bp, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_staging_bucket = \"41-processed-data-staging\"\n",
    "pb_partition = \"text-reuse/text-reuse_v1-0-0/boilerplate/\"\n",
    "\n",
    "s3 = get_s3_resource()\n",
    "bucket = s3.Bucket(s3_staging_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_key_name = os.path.join(pb_partition, bp_out_filename)\n",
    "bucket.upload_file(filtered_bp_pkl_out_filepath, bp_key_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity checking / loading existing bp.pkl file to check for specific ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_staging_bucket = \"\" # todo fill in\n",
    "pb_partition = \"text-reuse/text-reuse_v1-0-0/boilerplate/\"\n",
    "bp_dd_filename = '' # todo fill in with desired partition subpath\n",
    "dd_s3_path = os.path.join(\"s3://\", s3_staging_bucket, pb_partition, bp_dd_filename)\n",
    "dd_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the full df\n",
    "bp_df = pd.read_pickle(dd_s3_path, storage_options=IMPRESSO_STORAGEOPT).repartition(npartitions=2082).drop(columns=[\"is_boilerplate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_df[bp_df.id.str.contains('legaulois-1924-07-27-a')].head(100, npartitions=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_acquisition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
