{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boilerplate data preparation and computation\n",
    "\n",
    "When detecting text-reuse with passim, one can start by identifying and eliminating the boilerplate to allow to remove superfluous data from the processing.\n",
    "\n",
    "This notebook contains the code to perform various tasks relating to this:\n",
    "- Preparing the input data directory, only containing data that should be part of boilerplate detection\n",
    "- Light postprocessing of the boilerplate output and preparation of the actual text-reuse detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import jq\n",
    "import json\n",
    "from typing import Any\n",
    "from smart_open import open as smart_open_function\n",
    "import jsonlines\n",
    "import pickle\n",
    "\n",
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "from impresso_commons.utils.s3 import get_s3_resource, get_s3_client\n",
    "from impresso_commons.path.path_s3 import _list_bucket_paginator\n",
    "from impresso_commons.utils.s3 import IMPRESSO_STORAGEOPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_reuse_dir = '/scratch/piconti/impresso/text_reuse'\n",
    "all_rebuilt_data_path = \"rebuilt_data\"\n",
    "bp_rebuilt_data_path = \"rebuilt_data_for_bp\"\n",
    "\n",
    "input_dir = os.path.join(text_reuse_dir, all_rebuilt_data_path)\n",
    "output_dir = os.path.join(text_reuse_dir, bp_rebuilt_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_no_bp = [\n",
    "    \"FedGazDe\", \"FedGazFr\", \"NZZ\", \"handelsztg\", \"arbeitgeber\", \"ACI\", \"AV\", \"Bombe\", \"Cancoire\", \"Castigat\", \"Charivari\", \"CharivariCH\", \"CL\", \n",
    "    \"Croquis\", \"EM\", \"esta\", \"FAM\", \"FAMDE\", \"FAN\", \"FAV1\", \"Fronde\", \"GAVi\", \"Grelot\", \"Griffe\", \"Guepe1851\", \"Guepe1887\", \n",
    "    \"JH\", \"JV\", \"JVE\", \"JY2\", \"MB\", \"ME\", \"MESSAGER\", \"Moniteur\", \"NS\", \"NV\", \"NV1\", \"NV2\", \"OBS\", \"ouistiti\", \"pages\", \"PAT\", \"PDL\", \"PJ\", \"PS\", \"RLA\", \"TouSuIl\", \"VVS\", \"VVS1\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the copied Rebuilt data for Boilerplate detection with passim\n",
    "\n",
    "Before text-reuse detection with passim can be run, one must first run the tool in boilerplate mode to identify segments of text that should be excluded from the text-reuse search.\n",
    "\n",
    "However not all the corpus should be confronted to boilerplate detection, and titles without any article-level segmentation should not be considered.\n",
    "\n",
    "This small notebook aims to copy the wanted data (one that has article-level segmentation) to a new directory, where the files will be all together, as opposed to separated by title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copied, not_copied = [], []\n",
    "for path, dir, files in os.walk(input_dir):\n",
    "    if len(dir)==0:\n",
    "        if path.split('/')[-1] in titles_no_bp:\n",
    "            not_copied.extend(files)\n",
    "            print(f\"Not copying {path.split('/')[-1]} files since it has no article segmentation.\")\n",
    "        else:\n",
    "            print(f\"Copying {path.split('/')[-1]} files...\")\n",
    "            for file in tqdm(files):\n",
    "                src_path = os.path.join(path, file)\n",
    "                dest_path = os.path.join(output_dir, file)\n",
    "                shutil.copy(src_path, dest_path)\n",
    "                copied.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(copied), len(not_copied), len(copied) + len(not_copied)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload the current boilerplate output to s3\n",
    "\n",
    "For traceability, upload all the contents of the /out.json directory to S3 under a boilerplate partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_staging_bucket = \"41-processed-data-staging\"\n",
    "partition = \"text-reuse/text-reuse_v1-0-0/boilerplate/out.json\"\n",
    "out_jsons_dir = os.path.join(text_reuse_dir, \"passim_bp_output/out.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = get_s3_resource()\n",
    "bucket = s3.Bucket(s3_staging_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm(os.listdir(out_jsons_dir)):\n",
    "    if filename.endswith('json'):\n",
    "        filepath = os.path.join(out_jsons_dir, filename)\n",
    "        key_name = os.path.join(partition, filename)\n",
    "        bucket.upload_file(filepath, key_name)\n",
    "        #print(\"Uploaded %s to s3: %s\", filepath, key_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the bp.pkl file from the boilerplate output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First check the contents of some resulting jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_jsons_dir = os.path.join(text_reuse_dir, \"passim_bp_output/out.json\")\n",
    "\n",
    "os.listdir(out_jsons_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(f_path: str) -> dict:\n",
    "    lines = []\n",
    "    with open(f_path, mode=\"r\", encoding='utf-8') as f_in:\n",
    "        for line in f_in:\n",
    "            lines.append(json.loads(line))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually create the pb.pkl dataframe\n",
    "\n",
    "From looking at examples the following heuristics have been devised:\n",
    "- Only entries with the field `\"src\"` are actually boilerplate.\n",
    "  - In the format: `{\"id\": \"BDC-1839-03-18-a-i0005_1658_1952\", \"src\": {\"id\": \"BDC-1839-03-16-a-i0004\", \"start\": [...]`\n",
    "- For each entry with the field `\"src\"`:\n",
    "  - Two text passages are linked: the value of `id` and the value of `src.id`\n",
    "  - The value of `id` will often have additional values appended after the usual CI id (\"_1658_1952\" here). These should be removed\n",
    "  - Both ids (fields `id` and `src.id`) should be included in the bp.pkl output as separate rows\n",
    "- All rows should also have a column `\"is_boilerplate\"` set to `True`.\n",
    "\n",
    "The actual processing for this step of creating the bp.pkl dataframe has been moved to the `postprocess_boilerpalte.py` script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the resulting file to S3\n",
    "\n",
    "The upload of this dataframe has also been outsourced to the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_pkl_out_filepath = os.path.join(text_reuse_dir, \"bp.pkl\")\n",
    "s3_staging_bucket = \"41-processed-data-staging\"\n",
    "pb_partition = \"text-reuse/text-reuse_v1-0-0/boilerplate/\"\n",
    "\n",
    "s3 = get_s3_resource()\n",
    "bucket = s3.Bucket(s3_staging_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_pkl_out_filepath = os.path.join(text_reuse_dir, \"bp.pkl\")\n",
    "bp_key_name = os.path.join(pb_partition, \"bp.pkl\")\n",
    "bucket.upload_file(pb_pkl_out_filepath, bp_key_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug: Filtering the duplicates and uploading updated pkl to s3\n",
    "\n",
    "The first iteration of the bp.pkl dataframe was created without filtering out the duplicated ids. \n",
    "This caused some issues during the filtering, among other things due to the large size of the dataframe.\n",
    "The code filtering out duplicated ids had now also been added to the script\n",
    "\n",
    "Note: When using `compute()` after filtering, the output is of type pd.DataFrame and not dd.\n",
    "Trying both options and seeing which is best. --> decision was to keep it as a dask dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_staging_bucket = \"\" # todo fill in\n",
    "pb_partition = \"text-reuse/text-reuse_v1-0-0/boilerplate/\"\n",
    "bp_dd_filename = '' # todo fill in with desired partition subpath\n",
    "dd_s3_path = os.path.join(\"s3://\", s3_staging_bucket, pb_partition, bp_dd_filename)\n",
    "dd_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the full df\n",
    "bp_df = pd.read_pickle(dd_s3_path, storage_options=IMPRESSO_STORAGEOPT).repartition(npartitions=2082).drop(columns=[\"is_boilerplate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_df_filter = bp_df['id'].value_counts().map(lambda x: x > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_full_count = bp_df.count().compute()\n",
    "not_dupl = bp_df_filter[~bp_df_filter].count().compute()\n",
    "dupl = bp_df_filter[bp_df_filter].count().compute()\n",
    "\n",
    "bp_full_count, not_dupl, dupl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dup_bp = bp_df.drop_duplicates(subset=['id']).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dup_bp.head(), filtered_dup_bp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_reuse_dir = '/scratch/piconti/impresso/text_reuse'\n",
    "bp_out_filename = None # TODO fill in\n",
    "filtered_bp_pkl_out_filepath = os.path.join(text_reuse_dir, bp_out_filename)\n",
    "filtered_bp_pkl_out_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filtered_bp_pkl_out_filepath, 'wb') as handle:\n",
    "    pickle.dump(filtered_dup_bp, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_staging_bucket = \"41-processed-data-staging\"\n",
    "pb_partition = \"text-reuse/text-reuse_v1-0-0/boilerplate/\"\n",
    "\n",
    "s3 = get_s3_resource()\n",
    "bucket = s3.Bucket(s3_staging_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_key_name = os.path.join(pb_partition, bp_out_filename)\n",
    "bucket.upload_file(filtered_bp_pkl_out_filepath, bp_key_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity checking / loading existing bp.pkl file to check for specific ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_staging_bucket = \"\" # todo fill in\n",
    "pb_partition = \"text-reuse/text-reuse_v1-0-0/boilerplate/\"\n",
    "bp_dd_filename = '' # todo fill in with desired partition subpath\n",
    "dd_s3_path = os.path.join(\"s3://\", s3_staging_bucket, pb_partition, bp_dd_filename)\n",
    "dd_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the full df\n",
    "bp_df = pd.read_pickle(dd_s3_path, storage_options=IMPRESSO_STORAGEOPT).repartition(npartitions=2082).drop(columns=[\"is_boilerplate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_df[bp_df.id.str.contains('legaulois-1924-07-27-a')].head(100, npartitions=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepate the copied rebuilt data (without bolerplate) for actual text-reuse detection with passim\n",
    "\n",
    "Similarily as for step 1, the data should not be separated in sub-files for the passim text-reuse detection.\n",
    "As a result the data currently downloaded form s3 that is still not copied to a flat directory should (so all titles that did not undergo boilerplate detection).\n",
    "\n",
    "In particular, inside the `/scratch/piconti/impresso/text_reuse` directory:\n",
    "- `rebuilt_data` is the target directory where all jsonl files should be, both the ones resulting from boilerplate filtering and the ones to copy.\n",
    "- `rebuilt_data_to_mv` currently contains most of the data (organized per title) whihc did NOT undergo boilerplate detection/filtering. They should be copied/flatened into `rebuilt_data` along with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_reuse_dir = '/scratch/piconti/impresso/text_reuse'\n",
    "all_rebuilt_data_path = \"rebuilt_data\"\n",
    "separated_rebuilt_data_path = \"rebuilt_data_to_mv\"\n",
    "\n",
    "output_dir = os.path.join(text_reuse_dir, all_rebuilt_data_path)\n",
    "input_dir = os.path.join(text_reuse_dir, separated_rebuilt_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying FedGazDe from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:14<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying FedGazFr from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:15<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying NZZ from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [01:01<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Missing handelsztg! Will be uploaded from S3\n",
      "--------------\n",
      "Missing arbeitgeber! Will be uploaded from S3\n",
      "--------------\n",
      "Copying ACI from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 39.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying AV from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 103.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Missing Bombe! Will be uploaded from S3\n",
      "--------------\n",
      "Copying Cancoire from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 83.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying Castigat from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 164.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying Charivari from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 89.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying CharivariCH from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 295.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying CL from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 100.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying Croquis from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 1051.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying EM from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 82.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying esta from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:02<00:00, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying FAM from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 72.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying FAMDE from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 60.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying FAN from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 111.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying FAV1 from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 244.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying Fronde from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying GAVi from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 106.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying Grelot from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 246.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying Griffe from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 95.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying Guepe1851 from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 208.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying Guepe1887 from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 201.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying JH from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 71.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying JV from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 31.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying JVE from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 39.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying JY2 from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 82.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying MB from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:00<00:00, 303.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying ME from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 101.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying MESSAGER from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 136.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying Moniteur from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 624.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying NS from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 136.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying NV from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 72.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying NV1 from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 24.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying NV2 from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:18<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying OBS from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying ouistiti from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Missing pages! Will be uploaded from S3\n",
      "--------------\n",
      "Copying PAT from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 69.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying PDL from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying PJ from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 56.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying PS from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 548.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying RLA from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying TouSuIl from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:09<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying VVS from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 15.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Copying VVS1 from /scratch/piconti/impresso/text_reuse/rebuilt_data_to_mv to /scratch/piconti/impresso/text_reuse/rebuilt_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.20it/s]\n"
     ]
    }
   ],
   "source": [
    "titles_to_copy = os.listdir(input_dir)\n",
    "titles_to_upload = []\n",
    "copied = []\n",
    "\n",
    "for title in titles_no_bp:\n",
    "    print(f\"--------------\")\n",
    "    if title not in titles_to_copy:\n",
    "        titles_to_upload.append(title)\n",
    "        print(f\"Missing {title}! Will be uploaded from S3\")\n",
    "    else:\n",
    "        print(f\"Copying {title} from {input_dir} to {output_dir}\")\n",
    "        full_title_dir = os.path.join(input_dir, title)\n",
    "        for file in tqdm(os.listdir(full_title_dir)):\n",
    "            src_path = os.path.join(full_title_dir, file)\n",
    "            dest_path = os.path.join(output_dir, file)\n",
    "            shutil.copy(src_path, dest_path)\n",
    "            copied.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check that all files that need to be there are indeed.\n",
    "target = os.listdir(output_dir)\n",
    "missing = [f for path, dir, files in os.walk(input_dir) for f in files if f not in target and f.split('-')[0] in titles_no_bp]\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the titles that are still missing, look to s3 to download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_bucket = \"31-passim-rebuilt-staging\"\n",
    "s3_input_partition = \"passim/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['passim/Bombe/Bombe-1889.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1907.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1908.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1909.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1910.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1911.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1912.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1913.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1914.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1915.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1916.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1917.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1918.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1919.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1924.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1925.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1926.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1927.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1928.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1929.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1930.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1931.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1932.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1933.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1934.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1935.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1936.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1937.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1938.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1939.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1940.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1941.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1942.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1943.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1944.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1945.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1946.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1947.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1948.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1949.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1950.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1951.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1952.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1953.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1954.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1955.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1956.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1957.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1958.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1959.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1960.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1961.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1962.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1963.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1964.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1965.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1966.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1967.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1968.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1969.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1970.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1971.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1972.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1973.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1974.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1975.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1976.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1977.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1978.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1979.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1980.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1981.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1982.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1983.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1984.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1985.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1986.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1987.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1988.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1989.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1990.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1991.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1992.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1993.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1994.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1995.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1996.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1997.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1998.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-1999.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-2000.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-2001.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-2002.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-2003.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-2004.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-2005.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-2006.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-2007.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-2008.jsonl.bz2',\n",
       " 'passim/arbeitgeber/arbeitgeber-2010.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1861.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1862.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1863.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1864.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1865.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1866.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1867.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1868.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1869.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1870.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1871.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1872.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1873.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1874.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1875.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1876.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1877.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1878.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1879.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1880.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1881.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1882.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1883.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1884.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1885.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1886.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1890.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1891.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1892.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1893.jsonl.bz2',\n",
       " 'passim/handelsztg/handelsztg-1894.jsonl.bz2']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the accept condition for a key: the title is in titles_to_upload\n",
    "title_to_upload_key = lambda k: k.split('/')[-1].split('-')[0] in titles_to_upload\n",
    "\n",
    "keys = _list_bucket_paginator(s3_input_bucket, prefix=s3_input_partition, accept_key=title_to_upload_key)\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading passim/Bombe/Bombe-1889.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/Bombe-1889.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1907.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1907.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1908.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1908.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1909.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1909.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1910.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1910.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1911.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1911.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1912.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1912.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1913.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1913.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1914.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1914.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1915.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1915.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1916.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1916.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1917.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1917.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1918.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1918.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1919.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1919.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1924.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1924.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1925.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1925.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1926.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1926.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1927.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1927.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1928.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1928.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1929.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1929.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1930.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1930.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1931.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1931.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1932.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1932.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1933.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1933.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1934.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1934.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1935.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1935.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1936.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1936.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1937.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1937.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1938.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1938.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1939.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1939.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1940.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1940.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1941.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1941.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1942.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1942.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1943.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1943.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1944.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1944.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1945.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1945.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1946.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1946.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1947.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1947.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1948.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1948.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1949.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1949.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1950.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1950.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1951.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1951.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1952.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1952.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1953.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1953.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1954.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1954.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1955.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1955.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1956.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1956.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1957.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1957.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1958.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1958.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1959.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1959.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1960.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1960.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1961.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1961.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1962.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1962.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1963.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1963.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1964.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1964.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1965.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1965.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1966.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1966.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1967.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1967.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1968.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1968.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1969.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1969.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1970.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1970.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1971.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1971.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1972.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1972.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1973.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1973.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1974.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1974.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1975.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1975.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1976.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1976.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1977.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1977.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1978.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1978.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1979.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1979.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1980.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1980.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1981.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1981.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1982.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1982.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1983.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1983.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1984.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1984.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1985.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1985.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1986.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1986.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1987.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1987.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1988.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1988.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1989.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1989.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1990.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1990.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1991.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1991.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1992.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1992.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1993.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1993.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1994.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1994.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1995.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1995.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1996.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1996.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1997.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1997.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1998.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1998.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-1999.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-1999.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-2000.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-2000.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-2001.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-2001.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-2002.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-2002.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-2003.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-2003.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-2004.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-2004.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-2005.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-2005.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-2006.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-2006.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-2007.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-2007.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-2008.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-2008.jsonl.bz2\n",
      "downloading passim/arbeitgeber/arbeitgeber-2010.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/arbeitgeber-2010.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1861.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1861.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1862.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1862.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1863.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1863.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1864.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1864.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1865.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1865.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1866.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1866.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1867.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1867.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1868.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1868.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1869.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1869.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1870.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1870.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1871.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1871.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1872.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1872.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1873.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1873.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1874.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1874.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1875.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1875.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1876.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1876.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1877.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1877.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1878.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1878.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1879.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1879.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1880.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1880.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1881.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1881.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1882.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1882.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1883.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1883.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1884.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1884.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1885.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1885.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1886.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1886.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1890.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1890.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1891.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1891.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1892.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1892.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1893.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1893.jsonl.bz2\n",
      "downloading passim/handelsztg/handelsztg-1894.jsonl.bz2 to /scratch/piconti/impresso/text_reuse/rebuilt_data/handelsztg-1894.jsonl.bz2\n"
     ]
    }
   ],
   "source": [
    "for key in keys:\n",
    "    key_filename = os.path.split(key)[-1]\n",
    "    local_target_path = os.path.join(output_dir, key_filename)\n",
    "    print(f\"downloading {key} to {local_target_path}\")\n",
    "    get_s3_client().download_file(s3_input_bucket, key, local_target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pages']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check that all files that need to be there are indeed.\n",
    "target = os.listdir(output_dir)\n",
    "missing = [t for t in titles_no_bp if not any([t in f for f in target])]\n",
    "missing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_acquisition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
